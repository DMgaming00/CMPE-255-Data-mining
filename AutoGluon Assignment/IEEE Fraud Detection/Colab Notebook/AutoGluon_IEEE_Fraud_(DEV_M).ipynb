{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsuBNBW4JxPK"
      },
      "source": [
        "# âœ… AutoGluon â€” IEEE-CIS Fraud Detection\n",
        "\n"
      ],
      "id": "JsuBNBW4JxPK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assignmnet Submitted by :- **Dev Mulchandani**"
      ],
      "metadata": {
        "id": "xaDC31TNWwPm"
      },
      "id": "xaDC31TNWwPm"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLQyaG_OJxPL"
      },
      "source": [
        "%%capture\n",
        "!pip -q install -U pip\n",
        "!pip -q install -U autogluon kaggle\n",
        "print('Installed packages')"
      ],
      "id": "WLQyaG_OJxPL",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKKuIiHfJxPM"
      },
      "source": [
        "## ðŸ”‘ Kaggle API setup"
      ],
      "id": "dKKuIiHfJxPM"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "gTdfuofkJxPM",
        "outputId": "e8f1c55c-b8ea-451f-f314-705f8c2ff07a"
      },
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "print('Upload your kaggle.json (any name is fine) ...')\n",
        "uploaded = files.upload()\n",
        "assert len(uploaded) == 1, 'Please upload exactly one file.'\n",
        "src_name = next(iter(uploaded.keys()))\n",
        "os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "with open('/root/.kaggle/kaggle.json', 'wb') as f:\n",
        "    f.write(uploaded[src_name])\n",
        "os.chmod('/root/.kaggle/kaggle.json', 0o600)\n",
        "print('âœ… Token installed at /root/.kaggle/kaggle.json')\n",
        "!kaggle config view"
      ],
      "id": "gTdfuofkJxPM",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload your kaggle.json (any name is fine) ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1a22a676-8c3f-411b-ac2d-79709a3ea327\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1a22a676-8c3f-411b-ac2d-79709a3ea327\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle (2).json\n",
            "âœ… Token installed at /root/.kaggle/kaggle.json\n",
            "Configuration values from /root/.kaggle\n",
            "- username: dmgaming00\n",
            "- path: None\n",
            "- proxy: None\n",
            "- competition: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vON1eSj1JxPM"
      },
      "source": [
        "## âš™ï¸ Config"
      ],
      "id": "vON1eSj1JxPM"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTM_I7pmJxPM",
        "outputId": "e3a6e3fb-9d46-4bfd-8c5c-cc5fca024d45"
      },
      "source": [
        "COMPETITION = 'ieee-fraud-detection'\n",
        "LABEL = 'isFraud'\n",
        "TIME_LIMIT = 600\n",
        "MAX_ROWS = 200_000\n",
        "print('CONFIG OK')"
      ],
      "id": "eTM_I7pmJxPM",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CONFIG OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smfrBonfJxPM"
      },
      "source": [
        "## âœ… Check access & list files"
      ],
      "id": "smfrBonfJxPM"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9sSLHb0JxPM",
        "outputId": "3ebda7db-62f6-45ca-c224-3cf3ebe3b717"
      },
      "source": [
        "import subprocess, textwrap\n",
        "res = subprocess.run(['kaggle', 'competitions', 'files', '-c', COMPETITION], capture_output=True, text=True)\n",
        "if res.returncode != 0:\n",
        "    print(res.stderr)\n",
        "    raise SystemExit(textwrap.dedent('''\\\n",
        "âš ï¸ Kaggle access issue.\n",
        "â€¢ Join the competition and accept rules on the website.\n",
        "â€¢ Then re-run this cell.\n",
        "'''))\n",
        "print(res.stdout)"
      ],
      "id": "d9sSLHb0JxPM",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name                         size  creationDate                \n",
            "---------------------  ----------  --------------------------  \n",
            "sample_submission.csv     6080314  2019-07-15 00:19:01.536000  \n",
            "test_identity.csv        25797161  2019-07-15 00:19:01.536000  \n",
            "test_transaction.csv    613194934  2019-07-15 00:19:01.536000  \n",
            "train_identity.csv       26529680  2019-07-15 00:19:01.536000  \n",
            "train_transaction.csv   683351067  2019-07-15 00:19:01.536000  \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ciq-KdmJxPN"
      },
      "source": [
        "## â¬‡ï¸ Download & extract data"
      ],
      "id": "8Ciq-KdmJxPN"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zu5ow2AvJxPN",
        "outputId": "e069e3c4-7b74-49eb-c329-6940cd76cb04"
      },
      "source": [
        "import glob, zipfile, os\n",
        "\n",
        "print(\"Downloading from Kaggle...\")\n",
        "!kaggle competitions download -c ieee-fraud-detection -p /content/data\n",
        "\n",
        "print(\"Extracting ZIP files...\")\n",
        "for z in glob.glob('/content/data/*.zip'):\n",
        "    with zipfile.ZipFile(z, 'r') as f:\n",
        "        f.extractall('/content/data')\n",
        "\n",
        "print(\"âœ… Data ready in /content/data\")\n",
        "!ls -lh /content/data\n"
      ],
      "id": "Zu5ow2AvJxPN",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from Kaggle...\n",
            "ieee-fraud-detection.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Extracting ZIP files...\n",
            "âœ… Data ready in /content/data\n",
            "total 1.4G\n",
            "-rw-r--r-- 1 root root 119M Dec 11  2019 ieee-fraud-detection.zip\n",
            "-rw-r--r-- 1 root root 5.8M Nov  2 02:03 sample_submission.csv\n",
            "-rw-r--r-- 1 root root  25M Nov  2 02:03 test_identity.csv\n",
            "-rw-r--r-- 1 root root 585M Nov  2 02:03 test_transaction.csv\n",
            "-rw-r--r-- 1 root root  26M Nov  2 02:03 train_identity.csv\n",
            "-rw-r--r-- 1 root root 652M Nov  2 02:03 train_transaction.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Define base and data/model paths\n",
        "BASE = Path(\"/content\")          # or your Google Drive path if using USE_DRIVE=True\n",
        "DATA_DIR = BASE / \"data\"\n",
        "MODEL_DIR = BASE / \"AutoGluonModels\"\n",
        "\n",
        "# Make sure folders exist\n",
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Optional training limit and row cap for low-RAM\n",
        "TIME_LIMIT = 600\n",
        "MAX_ROWS = 200_000\n",
        "\n",
        "print(\"CONFIG OK\")\n",
        "print(\"DATA_DIR =\", DATA_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCdvK-DBLi_1",
        "outputId": "f8e74cf9-3668-4713-a979-6d4498b17367"
      },
      "id": "kCdvK-DBLi_1",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CONFIG OK\n",
            "DATA_DIR = /content/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zelCAX7JxPN"
      },
      "source": [
        "## ðŸ§¹ RAM-friendly preprocessing"
      ],
      "id": "-zelCAX7JxPN"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMShiSzPJxPN",
        "outputId": "b305929c-5b7d-43cb-9978-b7279668bd93"
      },
      "source": [
        "import pandas as pd, gc\n",
        "train_tr = pd.read_csv(DATA_DIR / 'train_transaction.csv')\n",
        "train_id = pd.read_csv(DATA_DIR / 'train_identity.csv')\n",
        "train = train_tr.merge(train_id, on='TransactionID', how='left')\n",
        "del train_tr, train_id; gc.collect()\n",
        "print('Raw train:', train.shape)\n",
        "long_text = [c for c in train.columns if train[c].dtype=='object' and train[c].astype(str).str.len().mean() > 30]\n",
        "DROP = set(long_text + ['TransactionID'])\n",
        "X = train.drop(columns=[c for c in DROP if c in train.columns])\n",
        "print('After drop:', X.shape, '| dropped:', len(DROP))\n",
        "if len(X) > MAX_ROWS:\n",
        "    X = X.sample(MAX_ROWS, random_state=0)\n",
        "print('After sample:', X.shape)"
      ],
      "id": "bMShiSzPJxPN",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw train: (590540, 434)\n",
            "After drop: (590540, 433) | dropped: 1\n",
            "After sample: (200000, 433)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1G5hT6nJxPN"
      },
      "source": [
        "## ðŸ§  Train (LightGBM only)"
      ],
      "id": "S1G5hT6nJxPN"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtCp-TWmJxPN",
        "outputId": "4dacb540-9f38-4193-fde3-25d0f4d5054f"
      },
      "source": [
        "from autogluon.tabular import TabularPredictor\n",
        "\n",
        "# Keep only valid model configs (remove None values)\n",
        "hp = {\n",
        "    'GBM': [{'num_boost_round': 200}],   # LightGBM only\n",
        "    # You can add others like 'RF': [{}], 'XT': [{}] later if you want\n",
        "}\n",
        "\n",
        "predictor = TabularPredictor(\n",
        "    label=LABEL,\n",
        "    eval_metric='roc_auc',\n",
        "    path=str(MODEL_DIR)\n",
        ").fit(\n",
        "    X,\n",
        "    hyperparameters=hp,\n",
        "    presets=None,\n",
        "    num_bag_folds=0,\n",
        "    num_stack_levels=0,\n",
        "    time_limit=TIME_LIMIT,\n",
        "    verbosity=2\n",
        ")\n",
        "\n",
        "predictor.fit_summary()\n"
      ],
      "id": "VtCp-TWmJxPN",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"/content/AutoGluonModels\"\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.4.0\n",
            "Python Version:     3.12.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Thu Oct  2 10:42:05 UTC 2025\n",
            "CPU Count:          2\n",
            "Memory Avail:       7.38 GB / 12.67 GB (58.2%)\n",
            "Disk Space Avail:   60.14 GB / 107.72 GB (55.8%)\n",
            "===================================================\n",
            "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
            "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
            "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
            "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
            "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
            "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
            "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
            "Beginning AutoGluon training ... Time limit = 600s\n",
            "AutoGluon will save models to \"/content/AutoGluonModels\"\n",
            "Train Data Rows:    200000\n",
            "Train Data Columns: 432\n",
            "Label Column:       isFraud\n",
            "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
            "\t2 unique label values:  [np.int64(0), np.int64(1)]\n",
            "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
            "Problem Type:       binary\n",
            "Preprocessing data ...\n",
            "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    7787.25 MB\n",
            "\tTrain Data (Original)  Memory Usage: 848.28 MB (10.9% of available memory)\n",
            "\tWarning: Data size prior to feature transformation consumes 10.9% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUnused Original Features (Count: 5): ['V28', 'V113', 'V118', 'V119', 'V305']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('float', []) : 5 | ['V28', 'V113', 'V118', 'V119', 'V305']\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 394 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n",
            "\t\t('int', [])    :   2 | ['TransactionDT', 'card1']\n",
            "\t\t('object', []) :  31 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  31 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n",
            "\t\t('float', [])    : 394 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n",
            "\t\t('int', [])      :   2 | ['TransactionDT', 'card1']\n",
            "\t26.7s = Fit runtime\n",
            "\t427 features in original data used to generate 427 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 610.37 MB (7.9% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 28.29s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
            "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.0125, Train Rows: 197500, Val Rows: 2500\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'GBM': [{'num_boost_round': 200}],\n",
            "}\n",
            "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBM ... Training model for up to 571.70s of the 571.70s of remaining time.\n",
            "\tFitting with cpus=1, gpus=0, mem=3.3/6.0 GB\n",
            "\t0.9296\t = Validation score   (roc_auc)\n",
            "\t51.43s\t = Training   runtime\n",
            "\t0.09s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 520.15s of remaining time.\n",
            "\tEnsemble Weights: {'LightGBM': 1.0}\n",
            "\t0.9296\t = Validation score   (roc_auc)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 83.34s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 27454.7 rows/s (2500 batch size)\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/content/AutoGluonModels\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Summary of fit() ***\n",
            "Estimated performance of each model:\n",
            "                 model  score_val eval_metric  pred_time_val   fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0             LightGBM   0.929648     roc_auc       0.090171  51.432822                0.090171          51.432822            1       True          1\n",
            "1  WeightedEnsemble_L2   0.929648     roc_auc       0.091059  51.438109                0.000888           0.005287            2       True          2\n",
            "Number of models trained: 2\n",
            "Types of models trained:\n",
            "{'LGBModel', 'WeightedEnsembleModel'}\n",
            "Bagging used: False \n",
            "Multi-layer stack-ensembling used: False \n",
            "Feature Metadata (Processed):\n",
            "(raw dtype, special dtypes):\n",
            "('category', []) :  31 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n",
            "('float', [])    : 394 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n",
            "('int', [])      :   2 | ['TransactionDT', 'card1']\n",
            "Plot summary of models saved to file: /content/AutoGluonModels/SummaryOfModels.html\n",
            "*** End of fit() summary ***\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model_types': {'LightGBM': 'LGBModel',\n",
              "  'WeightedEnsemble_L2': 'WeightedEnsembleModel'},\n",
              " 'model_performance': {'LightGBM': np.float64(0.9296483130171342),\n",
              "  'WeightedEnsemble_L2': np.float64(0.9296483130171342)},\n",
              " 'model_best': 'WeightedEnsemble_L2',\n",
              " 'model_paths': {'LightGBM': ['LightGBM'],\n",
              "  'WeightedEnsemble_L2': ['WeightedEnsemble_L2']},\n",
              " 'model_fit_times': {'LightGBM': 51.43282175064087,\n",
              "  'WeightedEnsemble_L2': 0.0052874088287353516},\n",
              " 'model_pred_times': {'LightGBM': 0.09017086029052734,\n",
              "  'WeightedEnsemble_L2': 0.0008881092071533203},\n",
              " 'num_bag_folds': 0,\n",
              " 'max_stack_level': 2,\n",
              " 'num_classes': 2,\n",
              " 'model_hyperparams': {'LightGBM': {'learning_rate': 0.05,\n",
              "   'num_boost_round': 200},\n",
              "  'WeightedEnsemble_L2': {'use_orig_features': False,\n",
              "   'valid_stacker': True,\n",
              "   'max_base_models': 0,\n",
              "   'max_base_models_per_type': 'auto',\n",
              "   'save_bag_folds': True,\n",
              "   'stratify': 'auto',\n",
              "   'bin': 'auto',\n",
              "   'n_bins': None}},\n",
              " 'leaderboard':                  model  score_val eval_metric  pred_time_val   fit_time  \\\n",
              " 0             LightGBM   0.929648     roc_auc       0.090171  51.432822   \n",
              " 1  WeightedEnsemble_L2   0.929648     roc_auc       0.091059  51.438109   \n",
              " \n",
              "    pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
              " 0                0.090171          51.432822            1       True   \n",
              " 1                0.000888           0.005287            2       True   \n",
              " \n",
              "    fit_order  \n",
              " 0          1  \n",
              " 1          2  }"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af_orP0NJxPN"
      },
      "source": [
        "## ðŸ”® Predict & create submission"
      ],
      "id": "af_orP0NJxPN"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCn15Iz8JxPN",
        "outputId": "cace3c75-1da1-4478-e2c0-92c26f759f6e"
      },
      "source": [
        "import pandas as pd, gc, os\n",
        "key = 'TransactionID'\n",
        "\n",
        "# 1) Raw training features (AutoGluon â‰¥1.4)\n",
        "needed_feats = list(predictor.feature_metadata.get_features())\n",
        "print(\"Total features expected by model:\", len(needed_feats))\n",
        "\n",
        "# 2) Map columns to files\n",
        "tx_head = pd.read_csv(DATA_DIR / 'test_transaction.csv', nrows=5)\n",
        "id_head = pd.read_csv(DATA_DIR / 'test_identity.csv', nrows=5)\n",
        "tx_cols_all, id_cols_all = set(tx_head.columns), set(id_head.columns)\n",
        "\n",
        "tx_usecols = sorted((set(needed_feats) & tx_cols_all) | {key})\n",
        "id_usecols = sorted((set(needed_feats) & id_cols_all) | {key})\n",
        "\n",
        "# 3) Load identity once (smaller)\n",
        "id_small = pd.read_csv(DATA_DIR / 'test_identity.csv', usecols=id_usecols).set_index(key)\n",
        "print(\"Identity small shape:\", id_small.shape)\n",
        "\n",
        "# 4) Prepare submission\n",
        "sub_path = DATA_DIR / 'my_submission.csv'\n",
        "with open(sub_path, 'w') as f:\n",
        "    f.write(f\"{key},{LABEL}\\n\")\n",
        "\n",
        "def to_positive_class_proba(p):\n",
        "    \"\"\"Return a 1-D numpy array of positive-class probabilities, regardless of shape/type.\"\"\"\n",
        "    import numpy as np\n",
        "    if isinstance(p, pd.DataFrame):\n",
        "        # Column names are class labels\n",
        "        pos = getattr(predictor, \"positive_class\", 1)\n",
        "        if pos in p.columns:\n",
        "            return p[pos].to_numpy()\n",
        "        else:\n",
        "            return p.iloc[:, -1].to_numpy()   # fallback: last col\n",
        "    if isinstance(p, pd.Series):\n",
        "        return p.to_numpy()\n",
        "    arr = np.asarray(p)\n",
        "    if arr.ndim == 2:\n",
        "        return arr[:, -1]                     # use last column as positive class\n",
        "    return arr\n",
        "\n",
        "# 5) Predict in chunks (adjust if RAM tight)\n",
        "chunksize = 100_000\n",
        "total_rows = 0\n",
        "\n",
        "for i, chunk in enumerate(pd.read_csv(DATA_DIR / 'test_transaction.csv',\n",
        "                                      usecols=tx_usecols,\n",
        "                                      chunksize=chunksize), 1):\n",
        "    # Join identity features\n",
        "    chunk = chunk.set_index(key).join(id_small, how='left').reset_index()\n",
        "\n",
        "    # Keep only model features\n",
        "    keep = [c for c in needed_feats if c in chunk.columns]\n",
        "    X_chunk = chunk[keep]\n",
        "\n",
        "    # If any expected features are missing, add them as NaN (AG can handle)\n",
        "    missing = [c for c in needed_feats if c not in X_chunk.columns]\n",
        "    for c in missing:\n",
        "        X_chunk[c] = float('nan')\n",
        "\n",
        "    # Ensure column order matches (not strictly required, but tidy)\n",
        "    X_chunk = X_chunk[[c for c in needed_feats]]\n",
        "\n",
        "    # Predict and always convert to 1-D positive-class proba\n",
        "    proba = to_positive_class_proba(predictor.predict_proba(X_chunk))\n",
        "\n",
        "    out = pd.DataFrame({key: chunk[key].values, LABEL: proba})\n",
        "    out.to_csv(sub_path, mode='a', header=False, index=False)\n",
        "\n",
        "    total_rows += len(out)\n",
        "    print(f\"Chunk {i}: wrote {len(out):,} rows (total {total_rows:,})\")\n",
        "    del chunk, X_chunk, proba, out\n",
        "    gc.collect()\n",
        "\n",
        "print(\"âœ… Saved submission to:\", sub_path)\n",
        "!ls -lh \"$sub_path\"\n",
        "!head -n 5 \"$sub_path\"\n",
        "\n"
      ],
      "id": "FCn15Iz8JxPN",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total features expected by model: 427\n",
            "Identity small shape: (141907, 2)\n",
            "Chunk 1: wrote 100,000 rows (total 100,000)\n",
            "Chunk 2: wrote 100,000 rows (total 200,000)\n",
            "Chunk 3: wrote 100,000 rows (total 300,000)\n",
            "Chunk 4: wrote 100,000 rows (total 400,000)\n",
            "Chunk 5: wrote 100,000 rows (total 500,000)\n",
            "Chunk 6: wrote 6,691 rows (total 506,691)\n",
            "âœ… Saved submission to: /content/data/my_submission.csv\n",
            "-rw-r--r-- 1 root root 14M Nov  2 02:13 /content/data/my_submission.csv\n",
            "TransactionID,isFraud\n",
            "3663549,0.004711710382252932\n",
            "3663550,0.008591421879827976\n",
            "3663551,0.019612561911344528\n",
            "3663552,0.0031595013570040464\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6UYxt5FJxPN"
      },
      "source": [
        "## ðŸš€ Optional submit"
      ],
      "id": "n6UYxt5FJxPN"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJtl-D6rJxPN"
      },
      "source": [
        "SUBMIT = False\n",
        "if SUBMIT:\n",
        "    import subprocess\n",
        "    subprocess.run(['kaggle', 'competitions', 'submit', '-c', COMPETITION, '-f', SUBMISSION_PATH, '-m', 'AutoGluon low-RAM baseline'], check=True)\n",
        "    print('Submission sent!')\n",
        "else:\n",
        "    print('Skipping submission (set SUBMIT=True to submit).')"
      ],
      "id": "hJtl-D6rJxPN",
      "execution_count": null,
      "outputs": []
    }
  ]
}