# âš™ï¸ 2. LoRA Parameter â€” Parameterâ€‘Efficient Fineâ€‘Tuning

This notebook fineâ€‘tunes only small adapter layers via **LoRA** instead of all parameters.

## ğŸš€ Steps
1. Load model and dataset.
2. Apply LoRA configuration (`r=8, alpha=16`).
3. Train and save adapter to `smollm2-lora-adapter/`.

## âœ… Benefits
- Lightweight training (~3Ã— less GPU memory).
- Fast execution on free Colab GPUs.
- Minimal quality loss vs full fineâ€‘tuning.
